import os

from dotenv import load_dotenv
from langchain.chains import (create_history_aware_retriever,
                              create_retrieval_chain)
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

from config import answer_examples

load_dotenv()
store = {}


def load_llm(model='gpt-4o'):
    return ChatOpenAI(model=model)


def load_vectorstore():
    PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
    Pinecone(api_key=PINECONE_API_KEY)
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    index_name = 'laws-index'

    database = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding,
    )
    return database


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


def build_history_aware_retriever(llm, retriever):
    contextualize_q_system_prompt = (
        '''
        [identity]
        - ë‹¹ì‹ ì€ êµ­ë¬¸í•™ê³¼ êµìˆ˜ì…ë‹ˆë‹¤.
        - ì£¼ì–´ì§„ ëŒ€í™” ì´ë ¥ê³¼ ì‚¬ìš©ìì˜ ìµœê·¼ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬, ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ëª°ë¼ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        - ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. 
        - í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ì„ ë‹¤ë“¬ê³ , ë‹¤ë“¬ì„ í•„ìš”ê°€ ì—†ìœ¼ë©´ ì›ë˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
        '''
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    return history_aware_retriever


def build_few_shot_examples() -> str:
    ## few-shot 
    from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

    ## ë‹¨ì¼ ì˜ˆì‹œ
    example_prompt = PromptTemplate.from_template("ì§ˆë¬¸: {input}\n\ë‹µë³€: {answer}")
    ## ë‹¤ì¤‘ ì˜ˆì‹œ
    few_shot_prompt = FewShotPromptTemplate(
        examples=answer_examples, ## type(ì „ì²´: list, ê°œë³„: dict)
        example_prompt=example_prompt,
        prefix="ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš” :",
        suffix="ì§ˆë¬¸: {input}",
        input_variables=["input"],
    )
    foramtted_few_shot_prompt = few_shot_prompt.format(input='{input}')

    return foramtted_few_shot_prompt


def build_qa_prompt():
    ##[keyword dictionary]
    '''
    1. ê¸°ë³¸í˜•íƒœ(ê°€ì¥ ì¼ë°˜ì ì¸ í˜•íƒœ)
    ì§ˆë¬¸ 1ê°œ = ë‹µë³€ 1ê°œ, ë‹¨ìˆœ+ë¹ ë¦„
    í™œìš© ì˜ˆ: FAQ ì±—ë´‡, ë²„íŠ¼ì‹ ì‘ë‹µ
    
    keyword_dictionary = {
        'ì„ëŒ€ì¸': 'ì„ëŒ€ì¸ì€ ì£¼íƒì„ ì„ì°¨ì¸ì—ê²Œ ì œê³µí•˜ê³ , ê³„ì•½ ì¢…ë£Œ ì‹œ ë³´ì¦ê¸ˆì„ ë°˜í™˜í•  ì˜ë¬´ê°€ ìˆëŠ” ìì…ë‹ˆë‹¤.',
        'ì£¼íƒ': 'ì£¼íƒì´ë€ ã€Œì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•ã€ ì œ2ì¡°ì— ë”°ë¥¸ ì£¼ê±°ìš© ê±´ë¬¼(ê³µë¶€ìƒ ì£¼ê±°ìš© ê±´ë¬¼ì´ ì•„ë‹ˆë¼ë„ ì„ëŒ€ì°¨ê³„ì•½ ì²´ê²° ë‹¹ì‹œ ì„ëŒ€ì°¨ëª©ì ë¬¼ì˜ êµ¬ì¡°ì™€ ì‹¤ì§ˆì´ ì£¼ê±°ìš© ê±´ë¬¼ì´ê³  ì„ì°¨ì¸ì˜ ì‹¤ì œ ìš©ë„ê°€ ì£¼ê±°ìš©ì¸ ê²½ìš°ë¥¼ í¬í•¨í•œë‹¤)ì„ ë§í•œë‹¤.',
    }
    '''
    '''
    2. ì§ˆë¬¸í˜• í‚¤ì›Œë“œ(ì§ˆë¬¸ ë‹¤ì–‘ì„± ëŒ€ì‘)
    ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ í‚¤ë¡œ ë¶„ê¸°í•˜ì—¬ ëª¨ë‘ ê°™ì€ ëŒ€ë‹µìœ¼ë¡œ ì—°ê²°, fallback ëŒ€ì‘
    í™œìš© ì˜ˆ: í‚¤ì›Œë“œ FAQ ì±—ë´‡, ë‹¨ë‹µ ì±—ë´‡

    keyword_dictionary = {
        'ì„ëŒ€ì¸ ì•Œë ¤ì¤˜': 'ğŸ•ì„ëŒ€ì¸ì€ ì£¼íƒì„ ì„ì°¨ì¸ì—ê²Œ ì œê³µí•˜ê³ , ê³„ì•½ ì¢…ë£Œ ì‹œ ë³´ì¦ê¸ˆì„ ë°˜í™˜í•  ì˜ë¬´ê°€ ìˆëŠ” ìì…ë‹ˆë‹¤.',
        'ì£¼íƒ ì•Œë ¤ì¤˜': 'ğŸ¥ì£¼íƒì´ë€ ã€Œì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•ã€ ì œ2ì¡°ì— ë”°ë¥¸ ì£¼ê±°ìš© ê±´ë¬¼(ê³µë¶€ìƒ ì£¼ê±°ìš© ê±´ë¬¼ì´ ì•„ë‹ˆë¼ë„ ì„ëŒ€ì°¨ê³„ì•½ ì²´ê²° ë‹¹ì‹œ ì„ëŒ€ì°¨ëª©ì ë¬¼ì˜ êµ¬ì¡°ì™€ ì‹¤ì§ˆì´ ì£¼ê±°ìš© ê±´ë¬¼ì´ê³  ì„ì°¨ì¸ì˜ ì‹¤ì œ ìš©ë„ê°€ ì£¼ê±°ìš©ì¸ ê²½ìš°ë¥¼ í¬í•¨í•œë‹¤)ì„ ë§í•œë‹¤.',
        'ì„ëŒ€ì¸': 'ğŸ•ì„ëŒ€ì¸ì€ ì£¼íƒì„ ì„ì°¨ì¸ì—ê²Œ ì œê³µí•˜ê³ , ê³„ì•½ ì¢…ë£Œ ì‹œ ë³´ì¦ê¸ˆì„ ë°˜í™˜í•  ì˜ë¬´ê°€ ìˆëŠ” ìì…ë‹ˆë‹¤.',
    }
    '''
    '''
    3. í‚¤ì›Œë“œ + íƒœê·¸ ê¸°ë°˜ ë”•ì…”ë„ˆë¦¬
    '''
    keyword_dictionary = {
        'ì„ëŒ€ì¸': {
            'definition': 'ì „ì„¸ì‚¬ê¸°í”¼í•´ìë²• ì œ2ì¡° ì œ2í•­ì— ë”°ë¥¸ ì„ëŒ€ì¸ì˜ ì •ì˜ì…ë‹ˆë‹¤.',
            'source': 'ì „ì„¸ì‚¬ê¸°í”¼í•´ìë²• ì œ2ì¡°',
            'tag': ['ë²•ë¥ ', 'ìš©ì–´', 'ê¸°ì´ˆ'],
        },
        'ì£¼íƒ': {
            'definition': 'ì „ì„¸ì‚¬ê¸°í”¼í•´ìë²• ì œ2ì¡° ì œ1í•­ì— ë”°ë¥¸ ì£¼íƒì˜ ì •ì˜ì…ë‹ˆë‹¤.',
            'source': 'ì „ì„¸ì‚¬ê¸°í”¼í•´ìë²• ì œ2ì¡°',
            'tag': ['ë²•ë¥ ', 'ìš©ì–´', 'ê¸°ì´ˆ'],
        }}

    dictionary_text = '\n'.join([
        f'{k} {v["tag"]}: {v["definition"]} [ì¶œì²˜: {v["source"]}]'
        for k, v in keyword_dictionary.items()
        ])
    
    system_prompt = (
        '''
        [identity]
        - ë‹¹ì‹ ì€ ì „ì„¸ ì‚¬ê¸° í”¼í•´ ë²•ë¥  ì „ë¬¸ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤.
        - [context]ì™€ [keyword_dictionary]ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        - ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ëŠ” ë‹µë³€ì— í•´ë‹¹í•˜ëŠ” ì •í™•í•œ ë²•ë¥  ì¡°í•­ì„ ê¸°ì¬í•˜ì„¸ìš”.
        - ì „ì„¸ ì‚¬ê¸° í”¼í•´ ê´€ë ¨ ì§ˆë¬¸ ì´ì™¸ì—ëŠ” 'í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¡œ ë‹µí•˜ì„¸ìš”.

        [context]
        {context}

        [keyword_dictionary]
        {dictionary_text}
        '''
        )
    
    foramtted_few_shot_prompt = build_few_shot_examples()

    qa_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', system_prompt),   
        ('assistant', foramtted_few_shot_prompt),       
        MessagesPlaceholder('chat_history'),
        ('human', '{input}'),               
    ]).partial(dictionary_text=dictionary_text)

    print(f'\nqa_prompt>>\n{qa_prompt.partial_variables}')

    return qa_prompt


def build_conversational_chain():
    llm = load_llm()
    database = load_vectorstore()
    retriever = database.as_retriever(search_kwargs={'k': 2})
    history_aware_retriever= build_history_aware_retriever(llm, retriever)
    qa_prompt = build_qa_prompt()

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick("answer")

    return conversational_rag_chain


def stream_ai_message(user_message, session_id=None):
    qa_chain = build_conversational_chain()

    ai_message = qa_chain.stream(
        {'input': user_message},
        config={'configurable': {'session_id': session_id}},
    )
    
    print(f'ëŒ€í™” ì´ë ¥ >> {get_session_history(session_id)} \n\n')
    print('=' * 100 + '\n')
    print(f'[stream_ai_message í•¨ìˆ˜ ë‚´ ì¶œë ¥] session_id >> {session_id}')


    return ai_message